{"cells":[{"cell_type":"markdown","source":["# Imports & Configuration"],"metadata":{"id":"RSyg5_fE4R6t"},"id":"RSyg5_fE4R6t"},{"cell_type":"code","execution_count":null,"id":"9567e892","metadata":{"id":"9567e892"},"outputs":[],"source":["!pip install wordsegment\n","!pip install num2words\n","!pip install skorch\n","\n","import os\n","import string\n","import nltk\n","import torch\n","import re\n","import spacy\n","import time\n","import torch\n","import torch.nn as nn\n","import random\n","import numpy as np\n","import seaborn as sns\n","import pandas as pd\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import wordsegment\n","import sklearn\n","from google.colab import drive\n","from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import GridSearchCV\n","from skorch.callbacks import Callback\n","from skorch.history import History\n","from imblearn.over_sampling import ADASYN\n","from sklearn.model_selection import KFold\n","from torch.optim import SGD\n","from gensim.models import KeyedVectors\n","from skorch.callbacks import LRScheduler\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","from skorch import NeuralNetClassifier\n","from tqdm import tqdm_notebook\n","from skorch.callbacks import EpochScoring\n","from num2words import num2words\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import KeyedVectors\n","from scipy.stats import randint\n","from gensim.models import FastText\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score, recall_score, precision_score, make_scorer\n","from skorch.callbacks import EarlyStopping\n","from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","\n","%matplotlib inline\n","\n","nltk.download(\"all\")\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","pd.set_option('display.max_colwidth', None)"]},{"cell_type":"code","source":["# setting the seed for reproducability\n","def set_seed(seed_value):\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","set_seed(42)"],"metadata":{"id":"xGJbPU7iR6MW","executionInfo":{"status":"ok","timestamp":1709686690085,"user_tz":-60,"elapsed":10,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"id":"xGJbPU7iR6MW","execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"id":"36bc5085","metadata":{"id":"36bc5085","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709686690086,"user_tz":-60,"elapsed":9,"user":{"displayName":"D C","userId":"13629916529612940322"}},"outputId":"4a3a497a-f27c-4f1f-fe4b-40cf1e5273ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Device name: Tesla T4\n"]}],"source":["# switch to gpu if available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","source":["# connect to the drive, remove if used on local device\n","drive.mount('/content/drive')"],"metadata":{"id":"1ltllMswB_06","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709686710583,"user_tz":-60,"elapsed":20505,"user":{"displayName":"D C","userId":"13629916529612940322"}},"outputId":"1f46612b-7890-4e79-afcf-fc3d43af4a2d"},"id":"1ltllMswB_06","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"id":"LLkzjFmQ3TLH"},"id":"LLkzjFmQ3TLH"},{"cell_type":"markdown","source":["## Loading Data"],"metadata":{"id":"R3wST1eRPD45"},"id":"R3wST1eRPD45"},{"cell_type":"code","execution_count":5,"id":"e7f1c41e","metadata":{"id":"e7f1c41e","executionInfo":{"status":"ok","timestamp":1709686715833,"user_tz":-60,"elapsed":5254,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Loading communication data, change this path when using this code on your local device\n","df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/communication_data.xlsm')"]},{"cell_type":"markdown","source":["## Cleaning Data"],"metadata":{"id":"p0aEubBzPImi"},"id":"p0aEubBzPImi"},{"cell_type":"code","execution_count":6,"id":"cc92cda6","metadata":{"id":"cc92cda6","executionInfo":{"status":"ok","timestamp":1709686715833,"user_tz":-60,"elapsed":6,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# lowercasing\n","df['Content'] = df['Content'].apply(lambda word: \" \".join(word.lower() for word in word.split()))"]},{"cell_type":"code","execution_count":7,"id":"81feb4fe","metadata":{"id":"81feb4fe","executionInfo":{"status":"ok","timestamp":1709686715833,"user_tz":-60,"elapsed":5,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# concatenating and reorganizing data\n","df = df.groupby('NegotiationID').agg({\n","    'NegoOutcome': 'first',\n","    'Content': lambda x: ' '.join(x)\n","}).reset_index()"]},{"cell_type":"code","execution_count":8,"id":"678cfba5","metadata":{"id":"678cfba5","executionInfo":{"status":"ok","timestamp":1709686870155,"user_tz":-60,"elapsed":153485,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Tokenization\n","spacy.require_gpu()\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# splitting words connected by punctuation\n","split_punctuation = string.punctuation + 'â‚¬'\n","split_punctuation_pattern = r'\\w*(?:['+split_punctuation+']+\\w*)+'\n","split_appended_pattern = pattern = r'(?<=[a-z])(?=[A-Z])'\n","\n","# converting numberes to text form\n","number_pattern = r'\\d+'\n","\n","# segmenting words that have been accidentally written together\n","wordsegment.load()\n","\n","def split_connected_words(text):\n","    split_text = re.sub(split_punctuation_pattern, lambda x: re.sub(r'['+split_punctuation+']+', lambda y: ' ' + y.group(0) + ' ', x.group(0)), text)\n","    return split_text\n","\n","def convert_numeric(text):\n","    converted_text = re.sub(number_pattern, lambda x: num2words(int(x.group(0))), text)\n","    return converted_text\n","\n","def word_segment(token_list):\n","    segmented_words = []\n","    for word in token_list:\n","        segmented_words.extend(wordsegment.segment(word))\n","    return segmented_words\n","\n","def tokenize_row(text):\n","    return word_tokenize(text)\n","\n","# applying changes and tokenizing\n","df['Content'] = df['Content'].apply(split_connected_words)\n","df['Content'] = df['Content'].apply(convert_numeric)\n","df['Content'] = df['Content'].apply(tokenize_row)\n","df['Content'] = df['Content'].apply(word_segment)"]},{"cell_type":"code","execution_count":9,"id":"5a048bac","metadata":{"id":"5a048bac","executionInfo":{"status":"ok","timestamp":1709686870572,"user_tz":-60,"elapsed":419,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Normalization\n","\n","# removing defined punctuation\n","exclude_punctuation = '!?$%'\n","custom_punctuation = ''.join([char for char in string.punctuation if char not in exclude_punctuation])\n","\n","# removing stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","def remove_punctuation(tokens):\n","    return [token for token in tokens if token not in custom_punctuation]\n","\n","def remove_stopwords(tokens):\n","    return [token for token in tokens if token not in stop_words]\n","\n","# applying changes\n","df['Content'] = df['Content'].apply(remove_punctuation)\n","df['Content'] = df['Content'].apply(remove_stopwords)"]},{"cell_type":"code","execution_count":10,"id":"cd26b737","metadata":{"id":"cd26b737","executionInfo":{"status":"ok","timestamp":1709686940241,"user_tz":-60,"elapsed":69673,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# lemmatization\n","spacy.require_gpu()\n","spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n","\n","def lemmatize_row(tokens):\n","    return [token.lemma_ for token in nlp(\" \".join(tokens))]\n","\n","\n","df['Content'] = df['Content'].apply(lemmatize_row)"]},{"cell_type":"markdown","source":["## Creating Embeddings"],"metadata":{"id":"pi6CcP4KQcvz"},"id":"pi6CcP4KQcvz"},{"cell_type":"code","execution_count":11,"id":"2ab6bec1","metadata":{"id":"2ab6bec1","executionInfo":{"status":"ok","timestamp":1709686940683,"user_tz":-60,"elapsed":459,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# storing all sample labels in ordered array\n","labels = []\n","for index, row in df.iterrows():\n","    if 'FinalAccept' in str(row.values):\n","        labels.append(0)\n","    else:\n","        labels.append(1)\n","\n","# storing all tokenized samples in ordered array, building vocabulary and finding maximum sentence length\n","tokenized_samples = []\n","word2index = {}\n","max_length = 0\n","\n","# adding indexes for padding and unknown tokens for vocabulary\n","word2index['<pad>'] = 0\n","word2index['<unk>'] = 1\n","\n","index = 2\n","for row in range(len(df)):\n","    tokenized_sample = df.iloc[row]['Content']\n","    tokenized_samples.append(tokenized_sample)\n","\n","    # add new tokens to vocabulary\n","    for token in tokenized_sample:\n","        if token not in word2index:\n","            word2index[token] = index\n","            index += 1\n","    max_length = max(max_length, len(tokenized_sample))"]},{"cell_type":"code","execution_count":12,"id":"f503c29b","metadata":{"id":"f503c29b","executionInfo":{"status":"ok","timestamp":1709686940977,"user_tz":-60,"elapsed":307,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# padding each sequence to the maximum sentence length\n","# storing all tokenized samples in an ordered array, with indixes as tokens\n","indexed_samples = []\n","for tokenized_sample in tokenized_samples:\n","\n","    tokenized_sample += ['<pad>'] * (max_length - len(tokenized_sample))\n","\n","    indexed_sample = [word2index.get(token) for token in tokenized_sample]\n","    indexed_samples.append(indexed_sample)"]},{"cell_type":"code","execution_count":13,"id":"27acf722","metadata":{"id":"27acf722","executionInfo":{"status":"ok","timestamp":1709686977369,"user_tz":-60,"elapsed":36398,"user":{"displayName":"D C","userId":"13629916529612940322"}},"colab":{"base_uri":"https://localhost:8080/","height":174},"outputId":"b0bf1629-84fb-4a4f-e34f-e4104d089eae"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 9936 / 10195 pretrained vectors found.\n"]},{"output_type":"execute_result","data":{"text/plain":["' Use this for Word2Vec instead\\ndef load_pretrained_vectors(word2index, model_file, target_dim):\\n    # Load the Word2Vec model\\n    word2vec_model = KeyedVectors.load_word2vec_format(model_file, binary=True)\\n\\n    # Get the dimension of the embeddings\\n    d = word2vec_model.vector_size\\n\\n    # Initialize random embeddings\\n    embeddings = np.random.uniform(-0.25, 0.25, (len(word2index), d))\\n    embeddings[word2index[\\'<pad>\\']] = np.zeros((d,))\\n\\n    words_found = []\\n\\n    # Load pretrained vectors\\n    count = 0\\n    for word, index in word2index.items():\\n        if word in word2vec_model:\\n            count += 1\\n            embeddings[index] = word2vec_model[word]\\n\\n    print(f\"There are {count} / {len(word2index)} pretrained vectors found.\")\\n\\n    return embeddings\\n\\ntarget_dim = 300\\nembeddings = load_pretrained_vectors(word2index, \\'/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin\\', target_dim)\\nembeddings = torch.tensor(embeddings)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["# loading pretrained vectors and creating embeddings with FastText\n","def load_pretrained_vectors(word2index, vectorfile, target_dim):\n","    file = open(vectorfile, 'r', encoding='utf-8', newline=\"\\n\", errors=\"ignore\")\n","    n, d = map(int, file.readline().split())\n","\n","    # initializing random embeddings\n","    embeddings = np.random.uniform(-0.25, 0.25, (len(word2index), d))\n","    embeddings[word2index['<pad>']] = np.zeros((d,))\n","\n","    words_found = []\n","\n","    # loading pretrained vectors\n","    count = 0\n","    for line in file:\n","        tokens = line.rstrip().split(' ')\n","        word = tokens[0]\n","        if word in word2index:\n","            count += 1\n","            embeddings[word2index[word]] = np.array(tokens[1:], dtype=np.float32)\n","\n","    print(f\"There are {count} / {len(word2index)} pretrained vectors found.\")\n","\n","    return embeddings\n","\n","target_dim = 300\n","embeddings = load_pretrained_vectors(word2index, '/content/drive/MyDrive/Colab Notebooks/crawl-300d-2M-subword.vec', target_dim)\n","embeddings = torch.tensor(embeddings)\n","\n","\n","''' Use this for Word2Vec instead\n","def load_pretrained_vectors(word2index, model_file, target_dim):\n","    # Load the Word2Vec model\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_file, binary=True)\n","\n","    # Get the dimension of the embeddings\n","    d = word2vec_model.vector_size\n","\n","    # Initialize random embeddings\n","    embeddings = np.random.uniform(-0.25, 0.25, (len(word2index), d))\n","    embeddings[word2index['<pad>']] = np.zeros((d,))\n","\n","    words_found = []\n","\n","    # Load pretrained vectors\n","    count = 0\n","    for word, index in word2index.items():\n","        if word in word2vec_model:\n","            count += 1\n","            embeddings[index] = word2vec_model[word]\n","\n","    print(f\"There are {count} / {len(word2index)} pretrained vectors found.\")\n","\n","    return embeddings\n","\n","target_dim = 300\n","embeddings = load_pretrained_vectors(word2index, '/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin', target_dim)\n","embeddings = torch.tensor(embeddings)\n","'''"]},{"cell_type":"markdown","source":["# Convolutional Neural Network"],"metadata":{"id":"DDmvCbKfRRcb"},"id":"DDmvCbKfRRcb"},{"cell_type":"markdown","source":["## Hyperparameters"],"metadata":{"id":"uQ1hzVyVRbgK"},"id":"uQ1hzVyVRbgK"},{"cell_type":"code","source":["# Initialization of Hyperparameters.\n","# These parameteres are only for initialization for sklearn\n","# The model will refer to the defined search space in sklearn for training\n","\n","embed_dim = 300\n","max_norm = 5\n","freeze_embeddings = False\n","vocab_size = None\n","\n","# Architecture\n","num_filters = [50, 100, 300]\n","filter_sizes = [2,5,7]\n","dropout_rate = 0.5\n","\n","# Optimizer\n","learning_rate = 0.25\n","rho = 0.9\n","\n","# Activation function\n","activation_function = F.relu\n","\n","# Calculate weights inversely proportional to class frequencies\n","positive_samples =  105\n","negative_samples = 518\n","total_samples = positive_samples + negative_samples\n","\n","weight_negative = total_samples / (2 * negative_samples)\n","weight_positive = total_samples / (2 * positive_samples)\n","class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float)\n","class_weights.to(device)\n","\n","loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n","loss_fn.to(device)\n","\n","# Pooling function\n","pooling_function = F.max_pool1d\n","\n","# other\n","epochs = 6\n","stride = 1\n","padding = 0\n"],"metadata":{"id":"bw8JmoAIRew5","executionInfo":{"status":"ok","timestamp":1709686977370,"user_tz":-60,"elapsed":18,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"id":"bw8JmoAIRew5","execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Model Definition"],"metadata":{"id":"CHZ1XVhoRh3R"},"id":"CHZ1XVhoRh3R"},{"cell_type":"code","source":["# defining 1D Convolutional Neural Network for Text Classification\n","class CNN_NLP(nn.Module):\n","    def __init__(self,\n","                pretrained_embedding,\n","                freeze_embedding,\n","                vocab_size,\n","                embed_dim,\n","                filter_sizes,\n","                num_filters,\n","                num_classes,\n","                stride,\n","                padding,\n","                dropout,\n","                max_norm):\n","\n","\n","        super(CNN_NLP, self).__init__()\n","\n","        # defining embedding layer\n","        if pretrained_embedding is not None:\n","            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n","            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n","                                                         freeze=freeze_embedding)\n","        else:\n","            self.embed_dim = embed_dim\n","            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                         embedding_dim=embed_dim,\n","                                         padding_idx=0,\n","                                         max_norm=max_norm)\n","\n","\n","        # defining nn architecture\n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=self.embed_dim,\n","                 out_channels=num_filters[i],\n","                 kernel_size=filter_sizes[i],\n","                 stride=stride,\n","                 padding=padding)\n","            for i in range(len(filter_sizes))\n","        ])\n","        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","\n","\n","    # defining forward function for passing data through the neural network\n","    def forward(self, indexed_samples):\n","\n","        x_embed=None\n","\n","        # get embeddings from input_ids\n","        # Output shape: (b, max_len, embed_dim)\n","        x_embed = self.embedding(indexed_samples).float()\n","\n","        # permute dimensions of the tensor to fit CNN requirements\n","        # output shape: (b, embed_dim, max_len)\n","        x_reshaped = x_embed.permute(0, 2, 1)\n","\n","        # apply CNN and ReLu\n","        # output shape: (b, num_filters[i], L_out)\n","        x_conv_list = [activation_function(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n","\n","        # Max Pooling. Output Shape: (b, num_filters[i], 1)\n","        x_pool_list = [pooling_function(x_conv, kernel_size=x_conv.shape[2])\n","                       for x_conv in x_conv_list]\n","\n","        # concatenating pooled tensors to feed Fully Connected Layer\n","        # output shape (b, sum(num_filters))\n","        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],   # TODO should i maybe add sigmoid here?\n","                        dim=1)\n","\n","        # compute logits for forward pass\n","        # output shape: (b, n_classes)\n","        logits = self.fc(self.dropout(x_fc))\n","\n","        return logits"],"metadata":{"id":"Z7A6V7TGRk0Q","executionInfo":{"status":"ok","timestamp":1709686977370,"user_tz":-60,"elapsed":17,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"id":"Z7A6V7TGRk0Q","execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Executing, Tuning, and Validating"],"metadata":{"id":"gGSjhAW_10pF"},"id":"gGSjhAW_10pF"},{"cell_type":"code","source":["# initialize model, these parameters will not affect the hyperoptimization loop, sklearn needs an initialized model.\n","cnn = CNN_NLP(embeddings,\n","              freeze_embeddings,\n","              vocab_size,\n","              embed_dim,\n","              filter_sizes,\n","              num_filters,\n","              2,\n","              dropout_rate,\n","              stride,\n","              padding,\n","              max_norm\n","            )\n","\n","\n","# wrap model in sklearn wrapper for hyperparameter tuning\n","wrappedModel = NeuralNetClassifier(\n","    cnn,\n","    criterion=nn.CrossEntropyLoss(weight=class_weights),\n","    optimizer= optim.Adadelta,\n","    max_epochs=20,\n","    lr=0.1,\n","    device= device,\n","    callbacks=[\n","        EarlyStopping(patience=10, monitor='valid_loss', lower_is_better=True)\n","    ]\n",")\n","\n","# define hyperparameter searchspace, adjust at your will\n","param_grid = {\n","    'lr':[0.0001,0.001, 0.01, 0.1],\n","    'batch_size':[16,32,64],\n","    'module__dropout':[0,0.3,0.5],\n","    'module__filter_sizes':[[2,3,4],[3,4,5],[3,5,7],[2,5,10]],\n","    'module__num_filters':[[300,300,300],[400,400,400],[500,500,500]],\n","    'module__stride':[1,2],\n","    'module__pretrained_embedding':[embeddings],\n","    'module__freeze_embedding':[freeze_embeddings],\n","    'module__vocab_size':[vocab_size],\n","    'module__embed_dim':[embed_dim],\n","    'module__padding':[0],\n","    'module__max_norm':[10],\n","    'module__num_classes':[2]\n","}\n","\n","# define scorers for model results\n","precision_scorer = make_scorer(precision_score)\n","recall_scorer = make_scorer(recall_score)\n","f1_scorer = make_scorer(f1_score)\n","accuracy_scorer = make_scorer(accuracy_score)\n","\n","# Define Cross Fold Validation, adjust the number in the next block if changed\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# Define Random Search and Number of Iterations\n","random_search = RandomizedSearchCV(wrappedModel, param_grid, cv=cv, scoring={'precision': precision_scorer,'recall': recall_scorer,'f1': f1_scorer, 'accuracy':accuracy_scorer}, refit='f1', verbose=1, n_iter=1)"],"metadata":{"id":"1q4ZbYArmki3","executionInfo":{"status":"ok","timestamp":1709687140560,"user_tz":-60,"elapsed":4,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"id":"1q4ZbYArmki3","execution_count":17,"outputs":[]},{"cell_type":"code","source":["# perform automatic hyperparameter tuning with RandomSearchCV and print the results\n","random_search.fit(np.asarray(indexed_samples), np.asarray(labels))\n","\n","best_score = random_search.best_score_\n","print(\"Average f1 score of best best configuration over all of its folds: \", best_score)\n","\n","# Access the average precision from the results\n","average_precision = random_search.cv_results_['mean_test_precision'][random_search.best_index_]\n","print(\"Average Precision for the best hyperparameter selection: \", average_precision)\n","\n","# Access the average precision from the results\n","average_recall = random_search.cv_results_['mean_test_recall'][random_search.best_index_]\n","print(\"Average Recall for the best hyperparameter selection: \", average_recall)\n","\n","# Access the average precision from the results\n","average_accuracy = random_search.cv_results_['mean_test_accuracy'][random_search.best_index_]\n","print(\"Average accuracy for the best hyperparameter selection: \", average_accuracy)\n","\n","# Assuming 10 folds, adjust the number accordingly\n","f1_scores_for_folds = [random_search.cv_results_[f'split{i}_test_f1'][random_search.best_index_] for i in range(10)]\n","\n","# Print the F1 scores for each fold\n","totalscore=0\n","for fold, f1_score in enumerate(f1_scores_for_folds):\n","    totalscore+=f1_score\n","    print(f\"Fold {fold + 1} F1 Score: {f1_score}\")\n","\n","print(totalscore/10)\n","\n","best_params = random_search.best_params_\n","print(\"Best Hyperparameters:\", best_params)"],"metadata":{"id":"lqPoYQ-LutxE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c37c5c75-c21d-4ccb-f6cd-11bea12422f8"},"id":"lqPoYQ-LutxE","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m0.6897\u001b[0m       \u001b[32m0.8407\u001b[0m        \u001b[35m0.6886\u001b[0m  7.2063\n","      2        0.6899       0.8319        \u001b[35m0.6871\u001b[0m  7.2386\n","      3        0.6906       0.8319        \u001b[35m0.6857\u001b[0m  7.2329\n","      4        \u001b[36m0.6879\u001b[0m       0.8319        \u001b[35m0.6847\u001b[0m  7.2252\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["# Save the model if wanted\n","best_model_crossfold = random_search.best_estimator_\n","\n","torch.save(best_model_crossfold, '/content/drive/MyDrive/Colab Notebooks/YOUR_MODEL_NAME.pth')"],"metadata":{"id":"me_KAHhL1A2x"},"id":"me_KAHhL1A2x","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["RSyg5_fE4R6t","uQ1hzVyVRbgK","CHZ1XVhoRh3R","gGSjhAW_10pF"],"machine_shape":"hm","toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}