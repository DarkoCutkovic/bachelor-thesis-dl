{"cells":[{"cell_type":"markdown","metadata":{"id":"RSyg5_fE4R6t"},"source":["# Imports & Configuration"],"id":"RSyg5_fE4R6t"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9567e892"},"outputs":[],"source":["!pip install wordsegment\n","!pip install num2words\n","!pip install skorch\n","\n","import os\n","import string\n","import nltk\n","import torch\n","import re\n","import spacy\n","import time\n","import torch\n","import torch.nn as nn\n","import random\n","import numpy as np\n","import seaborn as sns\n","import pandas as pd\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import wordsegment\n","import sklearn\n","from google.colab import drive\n","from imblearn.over_sampling import SMOTE\n","from skorch.callbacks import Callback\n","from imblearn.over_sampling import ADASYN\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.optim import SGD\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","from skorch import NeuralNetClassifier\n","from tqdm import tqdm_notebook\n","from skorch.callbacks import EpochScoring\n","from num2words import num2words\n","from sklearn.metrics import f1_score, recall_score, precision_score, make_scorer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import KeyedVectors\n","from scipy.stats import randint\n","from gensim.models import FastText\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","from skorch.callbacks import EarlyStopping\n","from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","\n","%matplotlib inline\n","\n","nltk.download(\"all\")\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","pd.set_option('display.max_colwidth', None)"],"id":"9567e892"},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1709685698436,"user":{"displayName":"D C","userId":"13629916529612940322"},"user_tz":-60},"id":"xGJbPU7iR6MW"},"outputs":[],"source":["# setting the seed for reproducability\n","def set_seed(seed_value):\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","set_seed(42)"],"id":"xGJbPU7iR6MW"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1709685698436,"user":{"displayName":"D C","userId":"13629916529612940322"},"user_tz":-60},"id":"36bc5085","outputId":"e0b5f0cc-73b2-476f-b193-558538c6141e"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Device name: Tesla T4\n"]}],"source":["# switch to gpu if available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"id":"36bc5085"},{"cell_type":"code","execution_count":4,"metadata":{"id":"1ltllMswB_06","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709685715578,"user_tz":-60,"elapsed":17146,"user":{"displayName":"D C","userId":"13629916529612940322"}},"outputId":"fe6c1b61-f64a-40a8-c48a-92009198bc8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# connect to the drive, remove if used on local device\n","drive.mount('/content/drive')"],"id":"1ltllMswB_06"},{"cell_type":"markdown","metadata":{"id":"LLkzjFmQ3TLH"},"source":["# Data Preparation"],"id":"LLkzjFmQ3TLH"},{"cell_type":"markdown","metadata":{"id":"R3wST1eRPD45"},"source":["## Loading Data"],"id":"R3wST1eRPD45"},{"cell_type":"code","execution_count":6,"metadata":{"id":"e7f1c41e","executionInfo":{"status":"ok","timestamp":1709685800126,"user_tz":-60,"elapsed":4644,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Loading communication data, change this path when using this code on your local device\n","df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/communication_data.xlsm')"],"id":"e7f1c41e"},{"cell_type":"markdown","metadata":{"id":"p0aEubBzPImi"},"source":["## Cleaning Data"],"id":"p0aEubBzPImi"},{"cell_type":"code","execution_count":7,"metadata":{"id":"cc92cda6","executionInfo":{"status":"ok","timestamp":1709685800476,"user_tz":-60,"elapsed":352,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# lowercasing\n","df['Content'] = df['Content'].apply(lambda word: \" \".join(word.lower() for word in word.split()))"],"id":"cc92cda6"},{"cell_type":"code","execution_count":8,"metadata":{"id":"81feb4fe","executionInfo":{"status":"ok","timestamp":1709685800476,"user_tz":-60,"elapsed":6,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# concatenating and reorganizing data\n","df = df.groupby('NegotiationID').agg({\n","    'NegoOutcome': 'first',\n","    'Content': lambda x: ' '.join(x)\n","}).reset_index()"],"id":"81feb4fe"},{"cell_type":"code","execution_count":9,"metadata":{"id":"678cfba5","executionInfo":{"status":"ok","timestamp":1709685966203,"user_tz":-60,"elapsed":165456,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Tokenization\n","spacy.require_gpu()\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# splitting words connected by punctuation\n","split_punctuation = string.punctuation + 'â‚¬'\n","split_punctuation_pattern = r'\\w*(?:['+split_punctuation+']+\\w*)+'\n","split_appended_pattern = pattern = r'(?<=[a-z])(?=[A-Z])'\n","\n","# converting numberes to text form\n","number_pattern = r'\\d+'\n","\n","# segmenting words that have been accidentally written together\n","wordsegment.load()\n","\n","def split_connected_words(text):\n","    split_text = re.sub(split_punctuation_pattern, lambda x: re.sub(r'['+split_punctuation+']+', lambda y: ' ' + y.group(0) + ' ', x.group(0)), text)\n","    return split_text\n","\n","def convert_numeric(text):\n","    converted_text = re.sub(number_pattern, lambda x: num2words(int(x.group(0))), text)\n","    return converted_text\n","\n","def word_segment(token_list):\n","    segmented_words = []\n","    for word in token_list:\n","        segmented_words.extend(wordsegment.segment(word))\n","    return segmented_words\n","\n","def tokenize_row(text):\n","    return word_tokenize(text)\n","\n","# applying changes and tokenizing\n","df['Content'] = df['Content'].apply(split_connected_words)\n","df['Content'] = df['Content'].apply(convert_numeric)\n","df['Content'] = df['Content'].apply(tokenize_row)\n","df['Content'] = df['Content'].apply(word_segment)"],"id":"678cfba5"},{"cell_type":"code","execution_count":10,"metadata":{"id":"5a048bac","executionInfo":{"status":"ok","timestamp":1709685966787,"user_tz":-60,"elapsed":587,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Normalization\n","\n","# removing defined punctuation\n","exclude_punctuation = '!?$%'\n","custom_punctuation = ''.join([char for char in string.punctuation if char not in exclude_punctuation])\n","\n","# removing stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","def remove_punctuation(tokens):\n","    return [token for token in tokens if token not in custom_punctuation]\n","\n","def remove_stopwords(tokens):\n","    return [token for token in tokens if token not in stop_words]\n","\n","# applying changes\n","df['Content'] = df['Content'].apply(remove_punctuation)\n","df['Content'] = df['Content'].apply(remove_stopwords)"],"id":"5a048bac"},{"cell_type":"code","execution_count":11,"metadata":{"id":"cd26b737","executionInfo":{"status":"ok","timestamp":1709686042509,"user_tz":-60,"elapsed":75726,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# lemmatization\n","spacy.require_gpu()\n","spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n","\n","def lemmatize_row(tokens):\n","    return [token.lemma_ for token in nlp(\" \".join(tokens))]\n","\n","\n","df['Content'] = df['Content'].apply(lemmatize_row)"],"id":"cd26b737"},{"cell_type":"markdown","metadata":{"id":"pi6CcP4KQcvz"},"source":["## Creating Embeddings"],"id":"pi6CcP4KQcvz"},{"cell_type":"code","execution_count":12,"metadata":{"id":"2ab6bec1","executionInfo":{"status":"ok","timestamp":1709686042888,"user_tz":-60,"elapsed":386,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# storing all sample labels in ordered array\n","labels = []\n","for index, row in df.iterrows():\n","    if 'FinalAccept' in str(row.values):\n","        labels.append(0)\n","    else:\n","        labels.append(1)\n","\n","# storing all tokenized samples in ordered array, building vocabulary and finding maximum sentence length\n","tokenized_samples = []\n","word2index = {}\n","max_length = 0\n","\n","# adding indexes for padding and unknown tokens for vocabulary\n","word2index['<pad>'] = 0\n","word2index['<unk>'] = 1\n","\n","index = 2\n","for row in range(len(df)):\n","    tokenized_sample = df.iloc[row]['Content']\n","    tokenized_samples.append(tokenized_sample)\n","\n","    # add new tokens to vocabulary\n","    for token in tokenized_sample:\n","        if token not in word2index:\n","            word2index[token] = index\n","            index += 1\n","    max_length = max(max_length, len(tokenized_sample))"],"id":"2ab6bec1"},{"cell_type":"code","execution_count":13,"metadata":{"id":"f503c29b","executionInfo":{"status":"ok","timestamp":1709686043123,"user_tz":-60,"elapsed":238,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# padding each sequence to the maximum sentence length\n","# storing all tokenized samples in an ordered array, with indixes as tokens\n","indexed_samples = []\n","for tokenized_sample in tokenized_samples:\n","\n","    tokenized_sample += ['<pad>'] * (max_length - len(tokenized_sample))\n","\n","    indexed_sample = [word2index.get(token) for token in tokenized_sample]\n","    indexed_samples.append(indexed_sample)"],"id":"f503c29b"},{"cell_type":"code","execution_count":14,"metadata":{"id":"27acf722","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1709686084381,"user_tz":-60,"elapsed":41261,"user":{"displayName":"D C","userId":"13629916529612940322"}},"outputId":"4cd61634-e5b9-429c-b65f-ab34268764a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 9936 / 10195 pretrained vectors found.\n"]},{"output_type":"execute_result","data":{"text/plain":["' Use this for Word2Vec instead\\ndef load_pretrained_vectors(word2index, model_file, target_dim):\\n    # Load the Word2Vec model\\n    word2vec_model = KeyedVectors.load_word2vec_format(model_file, binary=True)\\n\\n    # Get the dimension of the embeddings\\n    d = word2vec_model.vector_size\\n\\n    # Initialize random embeddings\\n    embeddings = np.random.uniform(-0.25, 0.25, (len(word2index), d))\\n    embeddings[word2index[\\'<pad>\\']] = np.zeros((d,))\\n\\n    words_found = []\\n\\n    # Load pretrained vectors\\n    count = 0\\n    for word, index in word2index.items():\\n        if word in word2vec_model:\\n            count += 1\\n            embeddings[index] = word2vec_model[word]\\n\\n    print(f\"There are {count} / {len(word2index)} pretrained vectors found.\")\\n\\n    return embeddings\\n\\ntarget_dim = 300\\nembeddings = load_pretrained_vectors(word2index, \\'/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin\\', target_dim)\\nembeddings = torch.tensor(embeddings)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["# loading pretrained vectors and creating embeddings with FastText\n","def load_pretrained_vectors(word2index, vectorfile, target_dim):\n","    file = open(vectorfile, 'r', encoding='utf-8', newline=\"\\n\", errors=\"ignore\")\n","    n, d = map(int, file.readline().split())\n","\n","    # initializing random embeddings\n","    embeddings = np.random.uniform(-0.25, 0.25, (len(word2index), d))\n","    embeddings[word2index['<pad>']] = np.zeros((d,))\n","\n","    words_found = []\n","\n","    # loading pretrained vectors\n","    count = 0\n","    for line in file:\n","        tokens = line.rstrip().split(' ')\n","        word = tokens[0]\n","        if word in word2index:\n","            count += 1\n","            embeddings[word2index[word]] = np.array(tokens[1:], dtype=np.float32)\n","\n","    print(f\"There are {count} / {len(word2index)} pretrained vectors found.\")\n","\n","    return embeddings\n","\n","target_dim = 300\n","embeddings = load_pretrained_vectors(word2index, '/content/drive/MyDrive/Colab Notebooks/crawl-300d-2M-subword.vec', target_dim)\n","embeddings = torch.tensor(embeddings)\n","\n","\n","''' Use this for Word2Vec instead\n","def load_pretrained_vectors(word2index, model_file, target_dim):\n","    # Load the Word2Vec model\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_file, binary=True)\n","\n","    # Get the dimension of the embeddings\n","    d = word2vec_model.vector_size\n","\n","    # Initialize random embeddings\n","    embeddings = np.random.uniform(-0.25, 0.25, (len(word2index), d))\n","    embeddings[word2index['<pad>']] = np.zeros((d,))\n","\n","    words_found = []\n","\n","    # Load pretrained vectors\n","    count = 0\n","    for word, index in word2index.items():\n","        if word in word2vec_model:\n","            count += 1\n","            embeddings[index] = word2vec_model[word]\n","\n","    print(f\"There are {count} / {len(word2index)} pretrained vectors found.\")\n","\n","    return embeddings\n","\n","target_dim = 300\n","embeddings = load_pretrained_vectors(word2index, '/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin', target_dim)\n","embeddings = torch.tensor(embeddings)\n","'''"],"id":"27acf722"},{"cell_type":"markdown","metadata":{"id":"DDmvCbKfRRcb"},"source":["\n","\n","\n","# Long Short Term Memory Network"],"id":"DDmvCbKfRRcb"},{"cell_type":"markdown","metadata":{"id":"uQ1hzVyVRbgK"},"source":["## Hyperparameters"],"id":"uQ1hzVyVRbgK"},{"cell_type":"code","execution_count":15,"metadata":{"id":"bw8JmoAIRew5","executionInfo":{"status":"ok","timestamp":1709686084382,"user_tz":-60,"elapsed":9,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Initialization of Hyperparameters.\n","# These parameteres are only for initialization for sklearn\n","# The model will refer to the defined search space in sklearn for training\n","\n","embed_dim = 300\n","max_norm = 5\n","freeze_embeddings = False\n","vocab_size = None\n","\n","# Architecture\n","dropout_rate = 0.5\n","\n","# Optimizer\n","learning_rate = 0.25\n","rho = 0.9\n","\n","# Activation function\n","activation_function = F.relu\n","\n","# Loss function\n","\n","# Calculate weights inversely proportional to class frequencies\n","positive_samples = 518\n","negative_samples = 105\n","total_samples = positive_samples + negative_samples\n","\n","weight_negative = total_samples / (2 * negative_samples)\n","weight_positive = total_samples / (2 * positive_samples)\n","class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float)\n","class_weights.to(device)\n","\n","# Pooling function\n","pooling_function = F.max_pool1d\n","\n","# lstm parameters\n","dimension = 128\n","num_layers = 1"],"id":"bw8JmoAIRew5"},{"cell_type":"markdown","metadata":{"id":"CHZ1XVhoRh3R"},"source":["## Model Definition"],"id":"CHZ1XVhoRh3R"},{"cell_type":"code","execution_count":16,"metadata":{"id":"Z7A6V7TGRk0Q","executionInfo":{"status":"ok","timestamp":1709686085222,"user_tz":-60,"elapsed":1,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# Defining LSTM for Text Classification\n","class LSTM(nn.Module):\n","\n","    def __init__(self,\n","                 pretrained_embedding,\n","                 freeze_embedding,\n","                 vocab_size,\n","                 dropout,\n","                 max_norm,\n","                 embed_dim,\n","                 num_classes,\n","                 num_layers,\n","                 dimension):\n","\n","        super(LSTM, self).__init__()\n","\n","\n","        # defining embedding layer\n","        if pretrained_embedding is not None:\n","            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n","            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n","                                                         freeze=freeze_embedding)\n","        else:\n","            self.embed_dim = embed_dim\n","            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                         embedding_dim=self.embed_dim,\n","                                         padding_idx=0,\n","                                         max_norm=self.max_norm)\n","\n","        self.dimension = dimension\n","\n","        # defining nn architecture\n","        self.lstm = nn.LSTM(input_size=self.embed_dim,\n","                            hidden_size=dimension,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=False)\n","\n","        self.drop = nn.Dropout(p=dropout)\n","\n","        self.fc = nn.Linear(dimension, num_classes)\n","\n","\n","    def forward(self, indexed_samples):\n","\n","        x_embed=None\n","\n","        # get embeddings from input_ids\n","        # Output shape: (b, max_len, embed_dim)\n","        x_embed = self.embedding(indexed_samples).float()\n","\n","        # receive output\n","        # ouput shape: (b, max_length, hidden_size)\n","        output, _ = self.lstm(x_embed)\n","        last_hidden_state = output[:, -1, :]\n","\n","        # compute logits\n","        logits = self.fc(self.drop(last_hidden_state))\n","\n","        return logits"],"id":"Z7A6V7TGRk0Q"},{"cell_type":"markdown","metadata":{"id":"gGSjhAW_10pF"},"source":["## Executing, Tuning, and Validating"],"id":"gGSjhAW_10pF"},{"cell_type":"code","execution_count":17,"metadata":{"id":"1q4ZbYArmki3","executionInfo":{"status":"ok","timestamp":1709686329921,"user_tz":-60,"elapsed":242,"user":{"displayName":"D C","userId":"13629916529612940322"}}},"outputs":[],"source":["# initialize model, these parameters will not affect the hyperoptimization loop, sklearn needs an initialized model.\n","lstm = LSTM(embeddings, freeze_embeddings, vocab_size, dropout_rate, max_norm, embed_dim, 2, num_layers, dimension)\n","\n","# wrap model in sklearn wrapper for hyperparameter tuning\n","wrappedModel = NeuralNetClassifier(\n","    lstm,\n","    criterion=nn.CrossEntropyLoss(weight=class_weights),\n","    optimizer= optim.Adadelta,\n","    max_epochs=20,\n","    lr=0.1,\n","    device= device,\n","    callbacks=[\n","    EarlyStopping(patience=10, monitor='valid_loss', lower_is_better=True)\n","    ]\n",")\n","\n","# define hyperparameter searchspace, adjust at your will\n","param_grid = {\n","    'lr':[0.0001,0.001, 0.01],\n","    'batch_size':[16,32,64],\n","    'module__dropout':[0,0.3,0.5],\n","    'module__pretrained_embedding':[embeddings],\n","    'module__freeze_embedding':[freeze_embeddings],\n","    'module__vocab_size':[vocab_size],\n","    'module__max_norm':[10],\n","    'module__embed_dim':[embed_dim],\n","    'module__num_classes':[2],\n","    'module__num_layers':[num_layers],\n","    'module__dimension': [32, 64, 128, 256, 512]\n","}\n","\n","# define scorers for model results\n","precision_scorer = make_scorer(precision_score)\n","recall_scorer = make_scorer(recall_score)\n","f1_scorer = make_scorer(f1_score)\n","accuracy_scorer = make_scorer(accuracy_score)\n","\n","# Define Cross Fold Validation, adjust the number in the next block if changed\n","cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# Define Random Search and Number of Iterations\n","random_search = RandomizedSearchCV(wrappedModel, param_grid, cv=cv, scoring={'precision': precision_scorer,'recall': recall_scorer,'f1': f1_scorer, 'accuracy':accuracy_scorer}, refit='f1', verbose=1, n_iter=1)"],"id":"1q4ZbYArmki3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqPoYQ-LutxE"},"outputs":[],"source":["# perform automatic hyperparameter tuning with RandomSearchCV and print the results\n","random_search.fit(np.asarray(indexed_samples), np.asarray(labels))\n","\n","best_score = random_search.best_score_\n","print(\"average f1 score of best best configuration over all of its folds: \", best_score)\n","\n","# Access the average precision from the results\n","average_precision = random_search.cv_results_['mean_test_precision'][random_search.best_index_]\n","print(\"Average Precision for the best hyperparameter selection: \", average_precision)\n","\n","# Access the average precision from the results\n","average_recall = random_search.cv_results_['mean_test_recall'][random_search.best_index_]\n","print(\"Average Recall for the best hyperparameter selection: \", average_recall)\n","\n","# Access the average precision from the results\n","average_accuracy = random_search.cv_results_['mean_test_accuracy'][random_search.best_index_]\n","print(\"Average accuracy for the best hyperparameter selection: \", average_accuracy)\n","\n","# Assuming 10 folds, adjust the number accordingly\n","f1_scores_for_folds = [random_search.cv_results_[f'split{i}_test_f1'][random_search.best_index_] for i in range(10)]\n","\n","# Print the F1 scores for each fold\n","totalscore=0\n","for fold, f1_score in enumerate(f1_scores_for_folds):\n","    totalscore+=f1_score\n","    print(f\"Fold {fold + 1} F1 Score: {f1_score}\")\n","\n","print(totalscore/10)\n","\n","best_params = random_search.best_params_\n","print(\"Best Hyperparameters:\", best_params)"],"id":"lqPoYQ-LutxE"},{"cell_type":"code","source":["# Save the model if wanted\n","best_model_crossfold = random_search.best_estimator_\n","\n","torch.save(best_model_crossfold, '/content/drive/MyDrive/Colab Notebooks/YOUR_MODEL_NAME.pth')"],"metadata":{"id":"4X_byTwDUSTo"},"id":"4X_byTwDUSTo","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["RSyg5_fE4R6t","pi6CcP4KQcvz"],"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}